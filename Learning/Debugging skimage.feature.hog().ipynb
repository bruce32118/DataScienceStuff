{
 "metadata": {
  "name": "",
  "signature": "sha256:7a14dbc4411cb4684f419ceca517ee784fbb65b5e91d1d7da0dd8bc73df3d6d6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import skimage.feature\n",
      "import time\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train = np.array(pd.read_csv('../Kaggle Contests/Digit Recognizer/train.csv',header=0))\n",
      "test = np.array(pd.read_csv('../Kaggle Contests/Digit Recognizer/test.csv',header=0))\n",
      "X, y = train[:,1:], train[:,0]\n",
      "small_X, small_y = X[:10], y[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy import sqrt, pi, arctan2, cos, sin\n",
      "from scipy.ndimage import uniform_filter\n",
      "\n",
      "def assert_nD(array, ndim, arg_name='image'):\n",
      "    \"\"\"\n",
      "    Verify an array meets the desired ndims.\n",
      "    Parameters\n",
      "    ----------\n",
      "    array : array-like\n",
      "        Input array to be validated\n",
      "    ndim : int or iterable of ints\n",
      "        Allowable ndim or ndims for the array.\n",
      "    arg_name : str, optional\n",
      "        The name of the array in the original function.\n",
      "    \"\"\"\n",
      "    array = np.asanyarray(array)\n",
      "    msg = \"The parameter `%s` must be a %s-dimensional array\"\n",
      "    if isinstance(ndim, int):\n",
      "        ndim = [ndim]\n",
      "    if not array.ndim in ndim:\n",
      "        raise ValueError(msg % (arg_name, '-or-'.join([str(n) for n in ndim])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
      "        cells_per_block=(3, 3), visualise=False, normalise=False):\n",
      "    \"\"\"Extract Histogram of Oriented Gradients (HOG) for a given image.\n",
      "    Compute a Histogram of Oriented Gradients (HOG) by\n",
      "        1. (optional) global image normalisation\n",
      "        2. computing the gradient image in x and y\n",
      "        3. computing gradient histograms\n",
      "        4. normalising across blocks\n",
      "        5. flattening into a feature vector\n",
      "    Parameters\n",
      "    ----------\n",
      "    image : (M, N) ndarray\n",
      "        Input image (greyscale).\n",
      "    orientations : int\n",
      "        Number of orientation bins.\n",
      "    pixels_per_cell : 2 tuple (int, int)\n",
      "        Size (in pixels) of a cell.\n",
      "    cells_per_block  : 2 tuple (int,int)\n",
      "        Number of cells in each block.\n",
      "    visualise : bool, optional\n",
      "        Also return an image of the HOG.\n",
      "    normalise : bool, optional\n",
      "        Apply power law compression to normalise the image before\n",
      "        processing.\n",
      "    Returns\n",
      "    -------\n",
      "    newarr : ndarray\n",
      "        HOG for the image as a 1D (flattened) array.\n",
      "    hog_image : ndarray (if visualise=True)\n",
      "        A visualisation of the HOG image.\n",
      "    References\n",
      "    ----------\n",
      "    * http://en.wikipedia.org/wiki/Histogram_of_oriented_gradients\n",
      "    * Dalal, N and Triggs, B, Histograms of Oriented Gradients for\n",
      "      Human Detection, IEEE Computer Society Conference on Computer\n",
      "      Vision and Pattern Recognition 2005 San Diego, CA, USA\n",
      "    \"\"\"\n",
      "    image = np.atleast_2d(image)\n",
      "\n",
      "    \"\"\"\n",
      "    The first stage applies an optional global image normalisation\n",
      "    equalisation that is designed to reduce the influence of illumination\n",
      "    effects. In practice we use gamma (power law) compression, either\n",
      "    computing the square root or the log of each colour channel.\n",
      "    Image texture strength is typically proportional to the local surface\n",
      "    illumination so this compression helps to reduce the effects of local\n",
      "    shadowing and illumination variations.\n",
      "    \"\"\"\n",
      "\n",
      "    assert_nD(image, 2)\n",
      "\n",
      "    if normalise:\n",
      "        image = sqrt(image)\n",
      "\n",
      "    \"\"\"\n",
      "    The second stage computes first order image gradients. These capture\n",
      "    contour, silhouette and some texture information, while providing\n",
      "    further resistance to illumination variations. The locally dominant\n",
      "    colour channel is used, which provides colour invariance to a large\n",
      "    extent. Variant methods may also include second order image derivatives,\n",
      "    which act as primitive bar detectors - a useful feature for capturing,\n",
      "    e.g. bar like structures in bicycles and limbs in humans.\n",
      "    \"\"\"\n",
      "\n",
      "    if image.dtype.kind == 'u':\n",
      "        # convert uint image to float\n",
      "        # to avoid problems with subtracting unsigned numbers in np.diff()\n",
      "        image = image.astype('float')\n",
      "\n",
      "    gx = np.empty(image.shape, dtype=np.double)\n",
      "    gx[:, 0] = 0\n",
      "    gx[:, -1] = 0\n",
      "    gx[:, 1:-1] = image[:, 2:] - image[:, :-2]\n",
      "    gy = np.empty(image.shape, dtype=np.double)\n",
      "    gy[0, :] = 0\n",
      "    gy[-1, :] = 0\n",
      "    gy[1:-1, :] = image[2:, :] - image[:-2, :]\n",
      "\n",
      "    \"\"\"\n",
      "    The third stage aims to produce an encoding that is sensitive to\n",
      "    local image content while remaining resistant to small changes in\n",
      "    pose or appearance. The adopted method pools gradient orientation\n",
      "    information locally in the same way as the SIFT [Lowe 2004]\n",
      "    feature. The image window is divided into small spatial regions,\n",
      "    called \"cells\". For each cell we accumulate a local 1-D histogram\n",
      "    of gradient or edge orientations over all the pixels in the\n",
      "    cell. This combined cell-level 1-D histogram forms the basic\n",
      "    \"orientation histogram\" representation. Each orientation histogram\n",
      "    divides the gradient angle range into a fixed number of\n",
      "    predetermined bins. The gradient magnitudes of the pixels in the\n",
      "    cell are used to vote into the orientation histogram.\n",
      "    \"\"\"\n",
      "\n",
      "    magnitude = sqrt(gx**2 + gy**2)\n",
      "    orientation = arctan2(gy, gx) * (180 / pi) % 180\n",
      "\n",
      "    sy, sx = image.shape\n",
      "    cx, cy = pixels_per_cell\n",
      "    bx, by = cells_per_block\n",
      "    #print(\"image.shape = %s\" % (image.shape,))\n",
      "    \n",
      "    n_cellsx = int(np.floor(sx // cx))  # number of cells in x\n",
      "    n_cellsy = int(np.floor(sy // cy))  # number of cells in y\n",
      "\n",
      "    # compute orientations integral images\n",
      "    orientation_histogram = np.zeros((n_cellsy, n_cellsx, orientations))\n",
      "    subsample = np.index_exp[cy // 2:cy * n_cellsy:cy,\n",
      "                             cx // 2:cx * n_cellsx:cx]\n",
      "    for i in range(orientations):\n",
      "        #create new integral image for this orientation\n",
      "        # isolate orientations in this range\n",
      "\n",
      "        temp_ori = np.where(orientation < 180.0 / orientations * (i + 1),\n",
      "                            orientation, -1)\n",
      "        temp_ori = np.where(orientation >= 180.0 / orientations * i,\n",
      "                            temp_ori, -1)\n",
      "        # select magnitudes for those orientations\n",
      "        cond2 = temp_ori > -1\n",
      "        temp_mag = np.where(cond2, magnitude, 0)\n",
      "\n",
      "        temp_filt = uniform_filter(temp_mag, size=(cy, cx))\n",
      "        orientation_histogram[:, :, i] = temp_filt[subsample]\n",
      "\n",
      "    # now for each cell, compute the histogram\n",
      "    hog_image = None\n",
      "\n",
      "    if visualise:\n",
      "        from skimage import draw\n",
      "\n",
      "        radius = min(cx, cy) // 2 - 1\n",
      "        hog_image = np.zeros((sy, sx), dtype=float)\n",
      "        for x in range(n_cellsx):\n",
      "            for y in range(n_cellsy):\n",
      "                for o in range(orientations):\n",
      "                    centre = tuple([y * cy + cy // 2, x * cx + cx // 2])\n",
      "                    dx = radius * cos(float(o) / orientations * np.pi)\n",
      "                    dy = radius * sin(float(o) / orientations * np.pi)\n",
      "                    rr, cc = draw.line(int(centre[0] - dx),\n",
      "                                       int(centre[1] + dy),\n",
      "                                       int(centre[0] + dx),\n",
      "                                       int(centre[1] - dy))\n",
      "                    hog_image[rr, cc] += orientation_histogram[y, x, o]\n",
      "\n",
      "    \"\"\"\n",
      "    The fourth stage computes normalisation, which takes local groups of\n",
      "    cells and contrast normalises their overall responses before passing\n",
      "    to next stage. Normalisation introduces better invariance to illumination,\n",
      "    shadowing, and edge contrast. It is performed by accumulating a measure\n",
      "    of local histogram \"energy\" over local groups of cells that we call\n",
      "    \"blocks\". The result is used to normalise each cell in the block.\n",
      "    Typically each individual cell is shared between several blocks, but\n",
      "    its normalisations are block dependent and thus different. The cell\n",
      "    thus appears several times in the final output vector with different\n",
      "    normalisations. This may seem redundant but it improves the performance.\n",
      "    We refer to the normalised block descriptors as Histogram of Oriented\n",
      "    Gradient (HOG) descriptors.\n",
      "    \"\"\"\n",
      "\n",
      "    n_blocksx = (n_cellsx - bx) + 1\n",
      "    n_blocksy = (n_cellsy - by) + 1\n",
      "    #print(\"n_blocksx=n_cellsx-bx+1 = %d-%d+1 = %d,\\nn_blocksy = n_cellsy-by+1 = %d-%d+1 = %d\\n\" % \n",
      "    #    (n_cellsx,bx,n_blocksx,n_cellsy,by,n_blocksy))\n",
      "    normalised_blocks = np.zeros((n_blocksy, n_blocksx,\n",
      "                                  by, bx, orientations))\n",
      "\n",
      "    for x in range(n_blocksx):\n",
      "        for y in range(n_blocksy):\n",
      "            block = orientation_histogram[y:y + by, x:x + bx, :]\n",
      "            eps = 1e-5\n",
      "            normalised_blocks[y, x, :] = block / sqrt(block.sum()**2 + eps)\n",
      "\n",
      "    \"\"\"\n",
      "    The final step collects the HOG descriptors from all blocks of a dense\n",
      "    overlapping grid of blocks covering the detection window into a combined\n",
      "    feature vector for use in the window classifier.\n",
      "    \"\"\"\n",
      "\n",
      "    if visualise:\n",
      "        return normalised_blocks.ravel(), hog_image\n",
      "    else:\n",
      "        return normalised_blocks.ravel()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "start = time.time()\n",
      "n_records = 10000\n",
      "np.array([skimage.feature.hog(row) for row in X[:n_records].ravel().reshape(len(X[:n_records]),28,28)])\n",
      "print(\"Processed %d records in %0.3f seconds\" % (n_records,(time.time()-start)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Processed 10000 records in 6.209 seconds\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "negative dimensions are not allowed",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-18-336f65a7bd5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mapply_hog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mhog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapply_hog\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msmall_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmall_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0moutshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m     \u001b[1;31m#  if res is a number, then we have a smaller output array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-18-336f65a7bd5e>\u001b[0m in \u001b[0;36mapply_hog\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mapply_hog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mhog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapply_hog\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msmall_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msmall_X\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-4-82d087a1fac9>\u001b[0m in \u001b[0;36mhog\u001b[1;34m(image, orientations, pixels_per_cell, cells_per_block, visualise, normalise)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;31m#    (n_cellsx,bx,n_blocksx,n_cellsy,by,n_blocksy))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     normalised_blocks = np.zeros((n_blocksy, n_blocksx,\n\u001b[1;32m--> 164\u001b[1;33m                                   by, bx, orientations))\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_blocksx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mValueError\u001b[0m: negative dimensions are not allowed"
       ]
      }
     ],
     "prompt_number": 18
    }
   ],
   "metadata": {}
  }
 ]
}